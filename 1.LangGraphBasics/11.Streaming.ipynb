{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04ff91f",
   "metadata": {},
   "source": [
    "Streaming using .stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9138ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,Annotated\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display,Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51e8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781c02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2440d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[list,add_messages]\n",
    "\n",
    "def chatbot(state:State):\n",
    "    return {\"messages\":[llm.invoke(state['messages'])]}\n",
    "\n",
    "graph=StateGraph(State)\n",
    "\n",
    "graph.add_node(\"chatbot\",chatbot)\n",
    "\n",
    "graph.add_edge(START,\"chatbot\")\n",
    "graph.add_edge(\"chatbot\",END)\n",
    "\n",
    "graph_builder=graph.compile(checkpointer=memory)\n",
    "display(Image(graph_builder.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b78eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320a0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chatbot': {'messages': [AIMessage(content=\"Understood, 007. A name that certainly commands attention! It's a pleasure to make your acquaintance.\\n\\nHow can I assist you today? Ready for your next mission?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--cdf436c8-dcde-4335-892b-52f6e4ed502b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 949, 'total_tokens': 962, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 910}})]}}\n"
     ]
    }
   ],
   "source": [
    "for event in graph_builder.stream({\"messages\":f\"hey there, you can call me 007.\"},config=config,stream_mode=\"updates\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6044b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hey there, you can call me 007.', additional_kwargs={}, response_metadata={}, id='e8232220-bcf4-4796-a0f6-ba8cb00323c9'), AIMessage(content=\"Understood, 007. A name that certainly commands attention! It's a pleasure to make your acquaintance.\\n\\nHow can I assist you today? Ready for your next mission?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--cdf436c8-dcde-4335-892b-52f6e4ed502b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 949, 'total_tokens': 962, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 910}}), HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='15341e33-2b8c-4024-9a64-948587cd8173')]}\n",
      "{'messages': [HumanMessage(content='hey there, you can call me 007.', additional_kwargs={}, response_metadata={}, id='e8232220-bcf4-4796-a0f6-ba8cb00323c9'), AIMessage(content=\"Understood, 007. A name that certainly commands attention! It's a pleasure to make your acquaintance.\\n\\nHow can I assist you today? Ready for your next mission?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--cdf436c8-dcde-4335-892b-52f6e4ed502b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 949, 'total_tokens': 962, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 910}}), HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='15341e33-2b8c-4024-9a64-948587cd8173'), AIMessage(content='Excellent, 007. A mission perfectly suited for an agent of your caliber, delving into the intricacies of real-time data flow.\\n\\nYour target is `astream()`, a critical component for building responsive and dynamic AI applications.\\n\\n---\\n\\n### **MISSION BRIEFING: PROJECT ECHO - Asynchronous Stream Intercept**\\n\\n**MISSION TITLE:** Project Echo: Asynchronous Stream Intercept\\n\\n**OBJECTIVE:** Your primary objective, 007, is to fully comprehend and practically implement the asynchronous streaming capabilities provided by `litellm`, specifically focusing on how to receive and process real-time, streaming responses from large language models. While `litellm` doesn\\'t have a standalone function explicitly called `astream()`, it provides this functionality via its `acompletion` method with the `stream=True` parameter, which returns an asynchronous generator – effectively *the* stream. You will master intercepting this data flow.\\n\\n**BACKGROUND INTEL:**\\nIn the world of LLMs, responses can be lengthy. Traditional API calls wait for the *entire* response before delivering it, leading to noticeable delays for the end-user. `astream()` (or rather, asynchronous streaming in general) is the solution. It allows you to receive data chunks as they are generated by the model, providing immediate feedback and a much smoother user experience. It\\'s the difference between waiting for a full dossier to print versus receiving pages as they\\'re typed.\\n\\n**FIELD MANUAL & TACTICS:**\\n\\n**PHASE 1: INFILTRATION - Setup & Prerequisites**\\n\\n1.  **Install `litellm`:** If you haven\\'t already, secure the necessary tools.\\n    ```bash\\n    pip install litellm\\n    ```\\n2.  **API Key Procurement:** `litellm` acts as a universal adapter. You\\'ll need an API key for at least one LLM provider (e.g., OpenAI, Anthropic, Cohere). Set it as an environment variable for secure access.\\n    ```bash\\n    export OPENAI_API_KEY=\"sk-your-openai-key\"\\n    # Or ANTHROPIC_API_KEY, COHERE_API_KEY, etc.\\n    ```\\n\\n**PHASE 2: INTERCEPT PROTOCOL - Basic Streaming Implementation**\\n\\nYour core task is to write a Python script that uses `litellm.acompletion` to initiate an asynchronous stream and then process the incoming data chunks.\\n\\n```python\\nimport litellm\\nimport os\\nimport asyncio\\n\\n# Ensure your API key is set as an environment variable\\n# e.g., export OPENAI_API_KEY=\"sk-...\"\\n\\nasync def mission_echo_intercept():\\n    \"\"\"\\n    Initiates an asynchronous stream with an LLM and processes the incoming chunks.\\n    \"\"\"\\n    print(\"007: Initiating Project Echo - Asynchronous Stream Intercept...\")\\n\\n    try:\\n        # Request an asynchronous completion with streaming enabled\\n        # The \\'stream=True\\' parameter tells litellm to return an async generator\\n        response_stream = await litellm.acompletion(\\n            model=\"gpt-3.5-turbo\", # You can try other models like \"claude-3-opus-20240229\" if you have the key\\n            messages=[\\n                {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement in a short, concise paragraph. Focus on its core idea.\"},\\n                {\"role\": \"user\", \"content\": \"Then, in a separate paragraph, describe a potential real-world application or implication of quantum entanglement.\"}\\n            ],\\n            stream=True # This is the crucial setting for streaming\\n        )\\n\\n        full_response_content = \"\"\\n        print(\"\\\\n007: Receiving encrypted data stream (chunks incoming):\")\\n        print(\"------------------------------------------------------\")\\n\\n        # Iterate asynchronously over the response stream\\n        # Each \\'chunk\\' is a small piece of the overall response\\n        async for chunk in response_stream:\\n            # Check if the chunk contains content\\n            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\\n                content = chunk.choices[0].delta.content\\n                print(content, end=\"\", flush=True) # Print immediately without newline, flush buffer\\n                full_response_content += content\\n            \\n            # Optional: Add a small delay to simulate network latency for better observation\\n            # await asyncio.sleep(0.01) \\n\\n        print(\"\\\\n------------------------------------------------------\")\\n        print(\"007: Stream transmission complete.\")\\n        # print(f\"\\\\nFull decrypted message:\\\\n{full_response_content}\")\\n\\n    except Exception as e:\\n        print(f\"007: Mission Alert! An error occurred during stream intercept: {e}\")\\n        # Add more robust error handling if needed\\n\\n# Execute the mission\\nif __name__ == \"__main__\":\\n    asyncio.run(mission_echo_intercept())\\n\\n```\\n\\n**PHASE 3: KEY CONCEPTS DECRYPTED**\\n\\n*   **`async def` & `await`:** This mission operates in an asynchronous environment. `async def` declares an asynchronous function, and `await` pauses execution until an asynchronous operation (like `litellm.acompletion`) completes without blocking the entire program.\\n*   **`stream=True`:** This parameter is your command to the LLM API to send data back in a continuous flow of small chunks rather than waiting for the entire response to be generated.\\n*   **`response_stream` (The `astream` object):** When `stream=True` is set, `litellm.acompletion` returns an *asynchronous generator*. This is the \"stream\" you\\'re learning about. It doesn\\'t contain the full response immediately; instead, it\\'s an object you can iterate over to get pieces of data as they arrive.\\n*   **`async for chunk in response_stream:`:** This is the critical line. It allows you to iterate over the asynchronous generator. Each `chunk` object represents a small piece of the model\\'s output.\\n*   **`chunk.choices[0].delta.content`:** This is the path to extract the actual text content from each received chunk. The structure is consistent with OpenAI-style streaming responses.\\n*   **`end=\"\"` & `flush=True`:** When printing, `end=\"\"` prevents a newline after each chunk, allowing the text to appear as a continuous stream. `flush=True` ensures the text is immediately displayed on your console rather than being buffered.\\n\\n**PHASE 4: ADVANCED MANEUVERS (BONUS OBJECTIVES)**\\n\\n1.  **Full Response Reconstruction:** Modify the script to build the complete response string *after* the stream has finished, in addition to printing it in real-time.\\n2.  **Error Handling:** Implement more specific `try...except` blocks to catch potential API errors (e.g., invalid API key, rate limits) and gracefully handle them.\\n3.  **Model Experimentation:** Change the `model` parameter to another LLM supported by `litellm` (e.g., `claude-3-haiku-20240307`, `gemini/gemini-pro`) and observe the streaming behavior. *Ensure you have the respective API keys set.*\\n4.  **Token Counting (Post-Stream):** After the stream concludes, use `litellm.completion_cost_calculator` to estimate the cost or token usage based on the full response.\\n\\n**DEBRIEFING & REPORTING:**\\n\\nUpon completion of the mission, 007, you will:\\n\\n1.  Run the provided Python script.\\n2.  Observe the real-time output in your console, noting how the text appears character by character or word by word.\\n3.  Explain, in your own words, *why* this streaming approach is superior for user experience compared to a non-streaming call.\\n4.  If you tackled any bonus objectives, report your findings and demonstrate your enhanced code.\\n\\n**DEADLINE:** This mission is to be completed within the next 72 hours.\\n\\n---\\n\\nGood luck, 007. The future of interactive AI applications depends on your success in intercepting these silent streams. Your report awaits.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--9143952c-ed3c-42c1-b7f7-02b22c2d7ce6-0', usage_metadata={'input_tokens': 63, 'output_tokens': 3705, 'total_tokens': 3768, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1915}})]}\n"
     ]
    }
   ],
   "source": [
    "for event in graph_builder.stream({\"messages\":f\"Assign a mission to learn about astream()\"},config=config,stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272fcc1",
   "metadata": {},
   "source": [
    "Using astream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347618f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': {'messages': 'Assign a mission to learn about astream()'}}, 'name': 'LangGraph', 'tags': [], 'run_id': '1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'metadata': {'thread_id': '2'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='6a871652-a315-49c5-8011-481231dfad67')]}}, 'name': 'chatbot', 'tags': ['graph:step:1'], 'run_id': 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662', 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637'}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da']}\n",
      "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='6a871652-a315-49c5-8011-481231dfad67')]]}}, 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 1423, 'total_tokens': 1433, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='`langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 51, 'output_token_details': {'reasoning': 0}, 'total_tokens': 51, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`,', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in ll', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='m.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `async', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='io.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()`', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPrompt', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Template` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async`', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:**', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 47, 'output_token_details': {'reasoning': 0}, 'total_tokens': 47, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=') for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4:', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=\" multiple `astream()` calls concurrently? You don't need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=':\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 50, 'output_token_details': {'reasoning': 0}, 'total_tokens': 50, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except`', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 47, 'output_token_details': {'reasoning': 0}, 'total_tokens': 47, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LC', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 49, 'output_token_details': {'reasoning': 0}, 'total_tokens': 49, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='EL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 51, 'output_token_details': {'reasoning': 0}, 'total_tokens': 51, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 48, 'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=\" critical role in building highly responsive and user-friendly LLM-powered applications. You'll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'output_tokens': 47, 'output_token_details': {'reasoning': 0}, 'total_tokens': 47, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}})}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', chunk_position='last')}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   `langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`, and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in llm.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `asyncio.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()` with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPromptTemplate` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async` function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:** Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay) for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4: Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage multiple `astream()` calls concurrently? You don\\'t need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing:\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except` error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LCEL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its critical role in building highly responsive and user-friendly LLM-powered applications. You\\'ll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 3089, 'total_tokens': 3099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}}), 'input': {'messages': [[HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='6a871652-a315-49c5-8011-481231dfad67')]]}}, 'run_id': '1686e860-484a-4d2b-b94b-ff3fb6d3ef12', 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662', 'name': 'chatbot', 'tags': ['graph:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637'}, 'data': {'chunk': {'messages': [AIMessage(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   `langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`, and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in llm.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `asyncio.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()` with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPromptTemplate` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async` function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:** Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay) for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4: Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage multiple `astream()` calls concurrently? You don\\'t need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing:\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except` error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LCEL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its critical role in building highly responsive and user-friendly LLM-powered applications. You\\'ll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 3089, 'total_tokens': 3099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}})]}}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da']}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   `langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`, and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in llm.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `asyncio.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()` with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPromptTemplate` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async` function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:** Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay) for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4: Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage multiple `astream()` calls concurrently? You don\\'t need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing:\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except` error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LCEL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its critical role in building highly responsive and user-friendly LLM-powered applications. You\\'ll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 3089, 'total_tokens': 3099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}})]}, 'input': {'messages': [HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='6a871652-a315-49c5-8011-481231dfad67')]}}, 'run_id': 'dc8a1eac-f62e-4a6e-8a1b-a723c39f5662', 'name': 'chatbot', 'tags': ['graph:step:1'], 'metadata': {'thread_id': '2', 'langgraph_step': 1, 'langgraph_node': 'chatbot', 'langgraph_triggers': ('branch:to:chatbot',), 'langgraph_path': ('__pregel_pull', 'chatbot'), 'langgraph_checkpoint_ns': 'chatbot:db1e0431-3239-1e88-69db-9190570d2637'}, 'parent_ids': ['1bf4bed2-c609-44ca-86f8-56b79cdec5da']}\n",
      "{'event': 'on_chain_stream', 'run_id': '1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '2'}, 'data': {'chunk': {'chatbot': {'messages': [AIMessage(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   `langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`, and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in llm.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `asyncio.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()` with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPromptTemplate` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async` function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:** Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay) for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4: Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage multiple `astream()` calls concurrently? You don\\'t need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing:\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except` error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LCEL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its critical role in building highly responsive and user-friendly LLM-powered applications. You\\'ll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 3089, 'total_tokens': 3099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}})]}}}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='Assign a mission to learn about astream()', additional_kwargs={}, response_metadata={}, id='6a871652-a315-49c5-8011-481231dfad67'), AIMessage(content='Excellent! Your mission, should you choose to accept it, is to become a master of asynchronous streaming with `astream()` in the context of LangChain Expression Language (LCEL).\\n\\n---\\n\\n## Mission Briefing: Project Nightingale\\n\\n**Mission Title:** Operation: Asynchronous Flow Mastery – Unlocking Real-time LLM Interactions with `astream()`\\n\\n**Mission Objective:** To deeply understand, implement, and appreciate the power of `astream()` in LangChain Expression Language (LCEL) for building responsive, real-time LLM applications, simulating a dynamic chat experience.\\n\\n**Mission Commander:** You!\\n\\n**Estimated Time to Completion:** 2-4 hours (depending on prior async/LangChain experience)\\n\\n**Prerequisites:**\\n*   Solid understanding of Python basics.\\n*   Familiarity with `async` and `await` in Python.\\n*   Basic knowledge of LangChain concepts (LLMs, Chains).\\n*   An OpenAI API key (or another LLM provider like Anthropic, Cohere, etc.).\\n\\n**Gear Required:**\\n*   Python 3.9+\\n*   `langchain` library (`pip install langchain`)\\n*   `langchain-openai` (or relevant provider package, `pip install langchain-openai`)\\n*   `python-dotenv` (optional, but recommended for API keys: `pip install python-dotenv`)\\n*   An IDE (VS Code, PyCharm) or Jupyter Notebook environment.\\n\\n---\\n\\n### Mission Phases:\\n\\n#### Phase 1: Reconnaissance (Theoretical Foundations)\\n\\n**Goal:** Understand the \"Why\" and \"What\" of `astream()`.\\n\\n1.  **Read Up:**\\n    *   What is `astream()` in LangChain? How does it relate to `stream()` and `invoke()`?\\n    *   Why is asynchronous programming (`async`/`await`) crucial for `astream()`? (Think I/O, network requests, responsiveness).\\n    *   What kind of objects does `astream()` yield? (Hint: often `AIMessageChunk` or parsed string chunks).\\n    *   How does the `async for` loop work in Python, and why is it essential for consuming `astream()`?\\n\\n2.  **Document:** Jot down your understanding of these points. Create a small table comparing `invoke()`, `stream()`, and `astream()` across aspects like return type, blocking/non-blocking, and use cases.\\n\\n#### Phase 2: First Contact (Basic `astream()` with an LLM)\\n\\n**Goal:** Implement `astream()` with a raw LLM and observe its chunks.\\n\\n1.  **Setup Environment:**\\n    *   Create a new Python project.\\n    *   Set up your API key (e.g., `OPENAI_API_KEY` in a `.env` file and load with `dotenv`).\\n    *   Initialize an `OpenAI` or `ChatOpenAI` model.\\n\\n2.  **Direct LLM `astream()` Call:**\\n    *   Write an `async` function.\\n    *   Inside, call `llm.astream(\"Tell me a short story about a brave knight and a wise dragon.\")`.\\n    *   Use an `async for chunk in llm.astream(...):` loop.\\n    *   **Crucial Observation:** Print each `chunk` as it arrives. Notice its type and content.\\n    *   **Execution:** Run your `async` function using `asyncio.run()`.\\n\\n3.  **Experiment:**\\n    *   Change the prompt. Make it longer.\\n    *   Observe the speed and granularity of chunks.\\n\\n#### Phase 3: Chain Reaction (Integrating `astream()` with LCEL)\\n\\n**Goal:** Utilize `astream()` with a simple LCEL chain, demonstrating its power in parsing and streaming.\\n\\n1.  **Build a Simple LCEL Chain:**\\n    *   Define a `ChatPromptTemplate` (e.g., \"You are a helpful assistant. Answer the following question: {question}\").\\n    *   Initialize your `ChatOpenAI` model.\\n    *   Add a `StrOutputParser` to parse the `AIMessageChunk` into strings.\\n    *   Construct an LCEL chain: `prompt | llm | parser`.\\n\\n2.  **`astream()` the Chain:**\\n    *   Modify your `async` function to use `chain.astream({\"question\": \"Explain the concept of quantum entanglement in simple terms.\"})`.\\n    *   Use `async for chunk in chain.astream(...):`\\n    *   **Crucial Observation:** Print each `chunk`. What is its type now? How does it differ from Phase 2? (It should be a string, thanks to the parser!).\\n\\n3.  **Simulate Real-time Chat:**\\n    *   Inside your `async for` loop, instead of just `print(chunk)`, try `print(chunk, end=\"\", flush=True)`.\\n    *   Add a `time.sleep(0.01)` (or similar small delay) for a more visual effect if the chunks are too fast.\\n    *   Observe how the text appears character-by-character or word-by-word, mimicking a real-time chat interface.\\n\\n#### Phase 4: Advanced Maneuvers (Error Handling & Parallelism Hint)\\n\\n**Goal:** Understand basic error handling and consider advanced use cases.\\n\\n1.  **Error Handling:**\\n    *   Modify your `astream()` loop with a `try...except` block to gracefully handle potential errors during streaming (e.g., network issues, API errors).\\n    *   Simulate an error if possible (e.g., pass an invalid API key temporarily, though this might stop the stream immediately).\\n\\n2.  **Consider Parallel Streaming (Conceptual):**\\n    *   Imagine you need to stream responses from *two different chains* simultaneously. How would `asyncio.gather()` or similar `asyncio` constructs help you manage multiple `astream()` calls concurrently? You don\\'t need to implement this, just think about the pattern.\\n\\n---\\n\\n### Mission Deliverables:\\n\\n1.  **`astream_report.md`:** A markdown file containing:\\n    *   Your comparison table from Phase 1.\\n    *   A brief explanation of *why* `astream()` is important for modern LLM applications.\\n    *   Your observations from Phase 2 (what did the raw chunks look like?).\\n    *   Your observations from Phase 3 (how did LCEL and the parser change the chunks?).\\n2.  **`llm_streaming.py`:** Your Python script containing:\\n    *   The basic `astream()` example from Phase 2.\\n    *   The LCEL `astream()` example from Phase 3, including the real-time chat simulation.\\n    *   Your `try...except` error handling from Phase 4.\\n3.  **Screenshot/GIF:** A screenshot or short GIF of your terminal output demonstrating the real-time streaming effect from Phase 3.\\n\\n---\\n\\n### Bonus Challenges (Optional, for the truly brave):\\n\\n*   **Custom Output Parser:** Create a custom `BaseOutputParser` that processes `AIMessageChunk` objects in a unique way before yielding.\\n*   **Web Integration:** Integrate your `astream()` LCEL chain into a simple FastAPI or Streamlit application to serve the streaming response to a web client.\\n*   **Performance Metrics:** Try to measure the time it takes for the *first chunk* to arrive vs. the *entire response* to arrive using both `invoke()` and `astream()`.\\n\\n---\\n\\n### Debriefing:\\n\\nUpon successful completion of this mission, you will have a profound understanding of `astream()`, its practical implementation with LCEL, and its critical role in building highly responsive and user-friendly LLM-powered applications. You\\'ll be able to articulate its benefits and confidently integrate it into your future projects.\\n\\nGood luck, Agent! The future of interactive AI awaits.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'model_provider': 'google_genai'}, id='lc_run--1686e860-484a-4d2b-b94b-ff3fb6d3ef12', usage_metadata={'input_tokens': 10, 'output_tokens': 3089, 'total_tokens': 3099, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1376}})]}}, 'run_id': '1bf4bed2-c609-44ca-86f8-56b79cdec5da', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '2'}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "config2={\"configurable\":{\"thread_id\":\"2\"}}\n",
    "async for event in graph_builder.astream_events({\"messages\":f\"Assign a mission to learn about astream()\"},config=config2,version=\"v2\"):\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
